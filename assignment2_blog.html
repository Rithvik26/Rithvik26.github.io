<!DOCTYPE HTML>
<!--
	Massively by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Golthi Rithvik's Portfolio</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<style>
		img {
  transform: scale(0.7);
}
	</style>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper" class="fade-in">

				<!-- Intro -->
					<div id="intro">
						<h1>Naive Bayes Classifier Blog</h1>
						
						<ul class="actions">
							<li><a href="#header" class="button icon solid solo fa-arrow-down scrolly">Continue</a></li>
						</ul>
					</div>

				<!-- Header -->
					<header id="header">
						<a href="assignment2_blog.html" class="logo">Naive Bayes Classifier Blog</a>
					</header>

				<!-- Nav -->
					<nav id="nav">
						<ul class="links">
							<li class="active"><a href="index.html" >Golthi Rithvik</a></li>
					
						</ul>
						<ul class="icons">

							<li><a target="_blank" href="https://github.com/Rithvik26/Ford_dataset_nbc/blob/main/Ford_Classification2.ipynb"><span class="label">Jupyter Notebook</span></a></li>
							<li><a target="_blank" href="https://drive.google.com/file/d/1xdRk1F8_AtMzDQecroxtKMzPI85l8FM7/view?usp=sharing"><span class="label">Resume</span></a></li>
							
							<li><a target="_blank" href="https://www.linkedin.com/in/rithvik-golthi-92048b167/" class="icon brands fa-linkedin"><span class="label">Linkedin</span></a></li>
							
							<li><a target="_blank" href="https://github.com/Rithvik26" class="icon brands fa-github"><span class="label">GitHub</span></a></li>
						</ul>
					</nav>

				<!-- Main -->
					<div id="main">
							
                                <header> <h2>Getting Started</h2>
									<p>In this blog we are going to learn how to implement Naive Bayes Classifier on text from scratch without using library Naive bayes classifier. For this task we are going to use the dataset of Ford Sentence Classification Dataset which we have taken from kaggle.</p>
						<br>
                        
                        <h2>Dataset</h2>
						<p>I have used python3 to implement the model.I first downloaded the Ford Sentence Classification dataset consisting of New_Sentences with their corresponding labels from kaggle and placed this dataset in my github repository.Then I extracted this data from my repository using the below code.</p>
							<img style='height: 100%; width: 100%; object-fit: contain' src="images/a2/1.png">
                        <br>
                        
                        <h2>Libraries</h2>
                        <p>Firstly, before we start working on the dataset, we are going to import required libraries as shown in the below code snippet:</p>
                        <img style='height: 100%; width: 100%; object-fit: contain' src="images/a2/2.png">
                        <br>

						<h2>Data Preprocessing</h2>
						<h2>Removing Tags</h2>
						<p>As the first step of the data preprocessing, we are going to remove the HTML tags, URLs and non-alphanumeric characters from the text in New_Sentence column of the data. After removal of the above tags we will be removing the stopwords.</p>
						<p>Before getting into the code for removing tags and stopwords let us learn what are stopwords.</p>

						<h3>What are Stop words</h3>
<p>Stopwords are any words that do not significantly add to the meaning of a phrase. They can be safely ignored without affecting the sentence's meaning. These are some of the most popular short function words for some search engines, along with the, is, at, which, and on. Stop words might be problematic in this situation when looking for phrases that contain them, especially in names like "The Who" or "Take That."</p>                        

                        <br>
						<h3>When to remove stop words?</h3>
<p>Stop words should be eliminated from our corpus when performing text classification or sentiment analysis because they don't add any useful information to our models, however stop words are helpful when performing.</p>
						<p>Using the below code snippet we will remove the tags and stop words.</p>
                        <img style='height: 100%; width: 100%; object-fit: contain' src="images/a2/3.png">

<br>
<h2>Lemmatization</h2>
<p>[2]Lemmatization is substantially more computationally expensive than stemming because it is not a ruled-based operation. Knowing the part of speech of tokens like "verb," "adverb," and "noun" is necessary for lemmatization.With the Natural Language Tool Kit (NLTK), the "WordNetLemmatizer()" method should be used to accomplish lemmatization.</p>

<p>A word will be lemmatized using the "Nltk.stem.WordNetLemmatizer.lemmatize" technique according to its context and placement in the sentence. This can be seen from the below code snippet</p>
						
<img style='height: 100%; width: 100%; object-fit: contain' src="images/a2/4.png">
<br>
<h2>Encoding the Labels</h2>
<p>Here using the LabelEncoder() we are encoding the "Type" column which contains the 5 types of labels namely [“Responsibility”,”Skill”,”SoftSkill”,”Requirement”,”Experience”] respectively with [0,1,2,3,4,5] for each record using the below code snippet. By Encoding the labels in this way will help us in the future for our Naive Bayes Classification.</p>						
<img style='height: 100%; width: 100%; object-fit: contain' src="images/a2/5.png">

<br>

						<h2>My Contribution</h2>
						<h2>Train-Development-Test Data Split</h2>
                        <p>Data is split into train,development and test sets. I made the decision to divide them in the ratio of 6:2:2 using the below code snippet.</p>
                        <img style='height: 100%; width: 100%; object-fit: contain' src="images/a2/6.png">
                        <br>
						<p>The total number of Training Examples and Labels are shown from using the below code snippet.</p>
                        <img style='height: 100%; width: 100%; object-fit: contain' src="images/a2/7.png">
						<br>
						<p>The total number of Development Examples and Labels are shown from using the below code snippet.</p>
                        <img style='height: 100%; width: 100%; object-fit: contain' src="images/a2/8.png">
						<br>
						<p>The total number of Test Examples and Labels are shown from using the below code snippet.</p>
                        <img style='height: 100%; width: 100%; object-fit: contain' src="images/a2/9.png">
						<br>
						<h2>Word Counts</h2>
						<p>First, we are going to use CountVectorizer to convert a text into a vector based on the number of times each word appears in the text as a whole. When we have several of these texts and want to turn each word into a vector, this is useful. We get the vocab from feature names picked from the raw documents using the "get feature names()" function.</p>
						<p>Finally, I have created a count_words function which return the word_counts based on the training sentences and labels as shown in the below code snippet.</p>
						<img style='height: 100%; width: 100%; object-fit: contain' src="images/a2/10.png">
						<p>Before getting started on implementing the Naive Bayes Classifier let us learn few things about Naive Bayes Classifier.</p>
<br>
						<h2>Naive Bayes Classifier:</h2>
						<img style='height: 100%; width: 100%; object-fit: contain' src="images/a2/11.png">
						[3]JaganDeep Singh : Naive bayes classification[https://jagan-singhh.medium.com/naive-bayes-classifier-99e3e618f8db]
						<p>The Bayes theorem is used in the Naive Bayes Classifier, a classification algorithm. It does NLP tasks like sentimental analysis with excellent results. A quick and simple classification algorithm is used. The application of this algorithm in NLP is extremely common.</p>
						<br>
						<h2>Bayes Theorem</h2>
						<p>A rational method for determining a conditional probability is the Bayes Theorem. The likelihood that something will happen provided that something else has already happened is known as the conditional probability. We can determine the likelihood that an event will occur given the information of a prior event by utilizing conditional probability.</p>
						<img style='height: 100%; width: 100%; object-fit: contain' src="images/a2/12.png">
						<br>
						<h2>Naive Bayes</h2>
						<p>Given the values of the features, the aim in the supervised learning method naive bayes for classification is to determine the class of the observation (data point). A naive bayes classifier (p(yi | x1, x2,..., xn)) determines the likelihood of a class given a set of feature values.</p>
						<img style='height: 100%; width: 100%; object-fit: contain' src="images/a2/13.png">
						<p>The probability of an occurrence is determined by the Naive Bayes classifier in the following ways:</p>
						<p>Step 1: determine the prior probability for the specified class labels.
							Step 2: Calculate the likelihood probability for each class given each attribute.
							Step 3: Apply the Bayes formula to these values to determine the posterior probability.
							Step 4: Determine which class, given that the input belongs to the higher probability class, has a greater likelihood.</p>

						<h2>Naive Bayes Implementation from Scratch</h2>
						<p>I have created a class called NaiveBayes from scratch as shown in the below code snippet.</p>
						<img style='height: 100%; width: 100%; object-fit: contain' src="images/a2/14.png">

						<h3>def __init__(self,l):</h3>
						<p>The __init__ function is the constructor of this Naive Bayes Classifier which accepts the number of unique labels for this classification.</p>

						<h3>def groupby_lab(self,x,y):</h3>
						<p>The groupby_lab is the function that is used to group the New_Sentences by its labels.</p>

						<h3>def laplace_smoothing(self,voc,word_c,sent_l):</h3>
						<p>We are performing the laplace smoothing using the laplace_smoothing function.</p>
						<h3>What is laplace smoothing?</h3>
						<p>Laplace smoothing is a smoothing method that addresses the Naive Bayes issue of zero probability. Laplace smoothing allows us to depict P(w'|positive) as</p>
						<img style='height: 100%; width: 100%; object-fit: contain' src="images/a2/15.png">
						[1]Vaibhav Jayswal: Laplace smoothing in Naïve Bayes algorithm [https://towardsdatascience.com/laplace-smoothing-in-na%C3%AFve-bayes-algorithm-9c237a8bdece]
						<br>
						<h3>def fit(self,x,y):</h3>
						<p>The "fit" function takes x (New_Sentences) and y (labels) values to be fitted on and stores the number of New_Sentences with each label and the conditional probabilities. We are taking the help of groupby_lab function in the fit method.</p>
						<h3>def predict(self,voc,word_c,x):</h3>
						<p>The "predict" function is written which returns predictions on unseen test sentences. As you can see in the predict function we are applying the laplace smoothing on each word before storing its final conditional word probability in each sentence.</p>
						<br>

						<h2>Creating a model</h2>
						<p>Next, I have created a model of Naive Bayes and fitted the model on the train_sentences using the below code Snippet. And, Initially I have made predictions on the development dataset as shown below.I have achieved an accuracy of 26% on the test dataset.</p>
						<img style='height: 100%; width: 100%; object-fit: contain' src="images/a2/16.png">
						<br>

						<h2>Testing the Model on Test Data</h2>
						<p>Finally, I have taken my trained model and tested it using the test_sentences as shown in the below code snippet. I have achieved an accuracy of 25% on the test dataset.</p>
						<img style='height: 100%; width: 100%; object-fit: contain' src="images/a2/17.png">
<br>
						
						<h2>References</h2>
						<p>
							<ol>
							<li>[1]Vaibhav Jayswal : Laplace smoothing in Naïve Bayes algorithm [<a href="https://towardsdatascience.com/laplace-smoothing-in-na%C3%AFve-bayes-algorithm-9c237a8bdece">https://towardsdatascience.com/laplace-smoothing-in-na%C3%AFve-bayes-algorithm-9c237a8bdece</a>]</li>
							<li>[2]GeeksforGeeks :Using CountVectorizer to Extracting Features from Tex [<a href="https://www.geeksforgeeks.org/using-countvectorizer-to-extracting-features-from-text/">https://www.geeksforgeeks.org/using-countvectorizer-to-extracting-features-from-text/</a>]</li>

							<li>[3]JaganDeep Singh : Naive bayes classification[<a href="https://jagan-singhh.medium.com/naive-bayes-classifier-99e3e618f8db">https://jagan-singhh.medium.com/naive-bayes-classifier-99e3e618f8db</a>]</li>

							<li>[4]Ford Sentence Classification Dataset : [<a href="https://www.kaggle.com/datasets/gaveshjain/ford-sentence-classifiaction-dataset?resource=download&select=test_data.csv">https://www.kaggle.com/datasets/gaveshjain/ford-sentence-classifiaction-dataset?resource=download&select=test_data.csv</a>]</li>
						</ol></p>

                        </header>
					</div>
		
			
				<!-- Footer -->
					<footer id="footer">
						
						<section class="split contact">
							<section class="alt">
								<h3>Address</h3>
								<p>
								Dallas, TX 75082</p>
							</section>
							
							<section>
								<h3>Email</h3>
								<p><a href="#">rxg9045@mavs.uta.edu</a></p>
							</section>
							<section>
								<h3>Social</h3>
								<ul class="icons alt">
									<li><a target="_blank" href="https://www.linkedin.com/in/rithvik-golthi-92048b167/" class="icon brands fa-linkedin"><span class="label">Linkedin</span></a></li>
							
							<li><a target="_blank" href="https://github.com/Rithvik26" class="icon brands fa-github"><span class="label">GitHub</span></a></li>
								</ul>
							</section>
						</section>
					</footer>

				<!-- Copyright -->
					<div id="copyright">
						<ul><li>&copy; Golthi</li></ul>
					</div>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>